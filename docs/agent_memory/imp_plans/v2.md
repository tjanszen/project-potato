# Implementation Plan v2 - Potato Runs & Totals Tracking

## Error Handling Clause
For all phases in this implementation plan:  
If critical issues (e.g., missing endpoints, >3 TypeScript errors, repeated server crashes, or unmet prerequisites) are discovered mid-phase, the Replit agent must:
1. Stop execution immediately
2. Provide a written summary of findings
3. Recommend next steps
4. Wait for user approval before resuming

## Overview
Phase-gated development plan for adding runs and totals tracking to the habit tracking web app. Users can track consecutive day runs and view comprehensive statistics. All features ship behind feature flags `ff.potato.runs_v2` and `ff.potato.totals_v2` (default OFF) with production-grade reliability and observability.

**Dependencies:** Requires v1 foundation (authentication, day marking, calendar interface) to be fully operational.

---

## Data Invariants & DB Constraints

### Core Data Invariants
All v2 implementations must maintain these testable invariants:

1. **No Overlapping Runs Per User**
   - Each user has at most one active run at any time
   - Historical runs do not overlap in date ranges
   - Constraint: `UNIQUE(user_id) WHERE active = true`

2. **Day Count Accuracy**  
   - `day_count = end_date - start_date + 1` for all runs
   - `end_date >= start_date` for all runs
   - Constraint: `CHECK (day_count = end_date - start_date + 1 AND end_date >= start_date)`

3. **Active Run Consistency**
   - The active run's `end_date` equals the user's most recent check-in date
   - No gaps between consecutive day_marks and active run boundaries
   - Verified by nightly reconciliation jobs

4. **Deterministic Rebuild**
   - Full rebuild from `day_marks` yields identical runs data
   - Algorithm is timezone-aware and idempotent
   - Property: `rebuild_user_runs(user_id) == current_runs(user_id)`

### Database Constraints Implementation

**PostgreSQL (Primary):**
```sql
-- Runs table with date range constraints
CREATE TABLE runs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id) NOT NULL,
  span daterange NOT NULL,
  day_count INTEGER NOT NULL,
  active BOOLEAN NOT NULL DEFAULT false,
  last_extended_at TIMESTAMP DEFAULT NOW(),
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW(),
  
  -- Generated helper columns
  start_date DATE GENERATED ALWAYS AS (lower(span)) STORED,
  end_date DATE GENERATED ALWAYS AS (upper(span) - 1) STORED,
  
  -- Invariant constraints
  CHECK (day_count = upper(span) - lower(span)),
  CHECK (upper(span) >= lower(span))
);

-- Non-overlapping date ranges per user
CREATE EXTENSION IF NOT EXISTS btree_gist;
EXCLUDE USING gist (user_id WITH =, span WITH &&) DEFERRABLE INITIALLY IMMEDIATE;

-- Single active run per user  
CREATE UNIQUE INDEX idx_runs_user_active ON runs(user_id) WHERE active = true;

-- Performance indexes
CREATE INDEX idx_runs_user_enddate ON runs(user_id, end_date DESC);
CREATE INDEX idx_runs_active_user ON runs(user_id, active);
```

**SQLite Fallback:**
```sql  
-- Runs table with trigger-based constraints
CREATE TABLE runs (
  id TEXT PRIMARY KEY DEFAULT (hex(randomblob(16))),
  user_id TEXT REFERENCES users(id) NOT NULL,
  start_date DATE NOT NULL,
  end_date DATE NOT NULL, 
  day_count INTEGER NOT NULL,
  active BOOLEAN NOT NULL DEFAULT 0,
  last_extended_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  
  -- Basic constraints
  CHECK (day_count = julianday(end_date) - julianday(start_date) + 1),
  CHECK (end_date >= start_date)
);

-- Single active run per user (SQLite 3.30+)
CREATE UNIQUE INDEX idx_runs_user_active ON runs(user_id) WHERE active = 1;

-- Overlap prevention trigger
CREATE TRIGGER prevent_overlaps
  BEFORE INSERT ON runs
  WHEN EXISTS (
    SELECT 1 FROM runs 
    WHERE user_id = NEW.user_id 
    AND NOT (NEW.end_date < start_date OR NEW.start_date > end_date)
  )
BEGIN
  SELECT RAISE(ABORT, 'Overlapping runs not allowed');
END;

-- Performance indexes  
CREATE INDEX idx_runs_user_enddate ON runs(user_id, end_date DESC);
CREATE INDEX idx_runs_active_user ON runs(user_id, active);
```

---

## Concurrency & Idempotency

### Unique Daily Fact Table
```sql
-- Ensure single source of truth for day marks
ALTER TABLE day_marks ADD CONSTRAINT unique_user_date UNIQUE(user_id, local_date);
```

### Per-User Serialization
- **PostgreSQL:** `SELECT ... FROM runs WHERE user_id = ? FOR UPDATE`
- **SQLite:** Advisory locks via application-level mutex per user_id
- **Scope:** Lock only user's runs, not entire table

### Idempotent Operations
- **Extend Run:** Duplicate day marking is no-op if run already includes date
- **Merge Runs:** Filling same gap multiple times produces identical result
- **Replay Safety:** Algorithm can be re-run without side effects

```sql
-- Example idempotent extend pseudocode
BEGIN TRANSACTION;
SELECT * FROM runs WHERE user_id = ? AND active = true FOR UPDATE;
IF new_date ALREADY IN active_run THEN ROLLBACK; -- No-op
IF new_date EXTENDS active_run THEN UPDATE runs SET end_date = new_date, day_count = day_count + 1;
COMMIT;
```

---

## Backfill/Rebuild & Shadow Cutover

### Admin Rebuild Routine
```bash
# rebuild_user_runs: Deterministic reconstruction from day_marks
rebuild_user_runs(user_id, from_date=null, to_date=null):
  1. BEGIN TRANSACTION
  2. DELETE FROM runs WHERE user_id = ? AND (from_date IS NULL OR start_date >= from_date)
  3. SELECT date FROM day_marks WHERE user_id = ? AND value = true ORDER BY date
  4. Group consecutive dates into runs using timezone-aware logic
  5. INSERT runs with calculated start_date, end_date, day_count, active=(end_date = today)
  6. COMMIT TRANSACTION
  7. Verify invariants hold post-rebuild
```

### Shadow Compute & Diff Detection
```sql
-- Compute v2 runs alongside legacy without affecting reads
CREATE TEMPORARY TABLE shadow_runs AS (
  SELECT user_id, start_date, end_date, day_count, active 
  FROM rebuild_user_runs(user_id) FOR ALL users
);

-- Generate diff reports
SELECT 'MISMATCH' as type, user_id, 'day_count' as field, 
       legacy.value as legacy_val, shadow.value as shadow_val
FROM legacy_stats legacy 
JOIN shadow_runs shadow USING (user_id)
WHERE legacy.total_days != shadow.total_days;

-- Alert thresholds: 0 mismatches required for cutover
```

### Cutover via Feature Flag
1. **Shadow Phase:** Compute v2 data, validate diffs = 0
2. **Read Cutover:** Enable `ff.potato.runs_v2`, dual-write to v2 + legacy
3. **Write Cutover:** Stop legacy writes, v2 becomes primary
4. **Cleanup:** Remove legacy code after 1 week stability

### Rollback Plan
- **Immediate:** Toggle `ff.potato.runs_v2 = false`
- **Legacy Reads:** Always available during shadow phase  
- **Data Recovery:** Rebuild from immutable `day_marks` table
- **Time to Recovery:** <5 minutes via feature flag, <1 hour via rebuild

---

## Edits/Deletions Behavior

### Check-in Removal (Future Feature)
```sql
-- When user removes day_marks entry:
DELETE FROM day_marks WHERE user_id = ? AND local_date = ?;

-- Run adjustment logic:
IF removed_date AT START of run THEN 
  UPDATE runs SET start_date = start_date + 1, day_count = day_count - 1
ELSE IF removed_date AT END of run THEN
  UPDATE runs SET end_date = end_date - 1, day_count = day_count - 1  
ELSE IF removed_date IN MIDDLE of run THEN
  -- Split run into two separate runs
  INSERT runs (start_date, end_date=removed_date-1, active=false)
  UPDATE runs SET start_date=removed_date+1
END IF
```

### Backfill Auto-Merge
```sql
-- When user marks day that closes gap between runs:
INSERT INTO day_marks (user_id, local_date, value) VALUES (?, ?, true);

-- Gap closure detection:
IF new_date CONNECTS run_a.end_date+1 AND run_b.start_date-1 THEN
  -- Merge three segments: run_a + new_date + run_b
  UPDATE runs SET end_date = run_b.end_date, day_count = run_a.day_count + 1 + run_b.day_count
  WHERE id = run_a.id;
  DELETE FROM runs WHERE id = run_b.id;
END IF
```

### State Transition Examples

**Month Boundary:**
- Dec 31 + Jan 1 = consecutive (timezone-aware)
- Handle leap year Feb 28/29 transitions correctly

**Timezone Policy:**
- `local_date` is fixed at mark time and never reinterpreted
- Past marks are not affected by user timezone changes
- Timezone migration requires administrative rebuild action only
- DST transitions: 23-hour and 25-hour days remain consecutive

**Edge Case Matrix:**
| Scenario | Before | Action | After |
|----------|--------|--------|-------|
| Split Run | Run(1-10, count=10) | Remove day 5 | Run(1-4, count=4) + Run(6-10, count=5) |
| Merge Runs | Run(1-3) + Run(5-7) | Mark day 4 | Run(1-7, count=7) |
| Month Cross | Run(Jan 30-31) | Mark Feb 1 | Run(Jan 30-Feb 1, count=3) |

---

## Totals Strategy

### Decision: Hybrid Approach
- **Real-time:** Current run and total days computed on-read (cached)
- **Stored:** Monthly aggregates for historical analysis and performance

### Stored Totals Schema
```sql
-- Feature flag: ff.potato.totals_v2
CREATE TABLE run_totals (
  user_id UUID REFERENCES users(id),
  year_month CHAR(7) NOT NULL, -- 'YYYY-MM' format
  total_days INTEGER NOT NULL DEFAULT 0,
  longest_run_days INTEGER NOT NULL DEFAULT 0,  
  active_run_days INTEGER DEFAULT NULL, -- NULL if no active run
  updated_at TIMESTAMP DEFAULT NOW(),
  
  PRIMARY KEY (user_id, year_month)
);

CREATE INDEX idx_totals_user_month ON run_totals(user_id, year_month DESC);
```

### Update Strategy  
```sql
-- Recompute current + prior month on any day marking change
UPDATE run_totals SET 
  total_days = (SELECT SUM(day_count) FROM runs WHERE user_id = ? AND EXTRACT(year_month FROM start_date) = ?),
  longest_run_days = (SELECT MAX(day_count) FROM runs WHERE user_id = ?),
  active_run_days = (SELECT day_count FROM runs WHERE user_id = ? AND active = true)
WHERE user_id = ? AND year_month IN (current_month, prior_month);

-- Nightly reconciliation job
SELECT user_id FROM run_totals 
WHERE updated_at < NOW() - INTERVAL '24 hours'
AND EXISTS (
  SELECT 1 FROM day_marks 
  WHERE user_id = run_totals.user_id 
  AND created_at > run_totals.updated_at
);
```

---

## Observability & SLIs

### Event Emission
```json
// Structured events for all run operations
{
  "event": "run_created",
  "user_id": "uuid",
  "run_id": "uuid", 
  "start_date": "2025-06-15",
  "end_date": "2025-06-15",
  "day_count": 1,
  "triggered_by": "day_mark_created",
  "timestamp": "2025-06-15T10:30:00Z",
  "correlation_id": "req-123"
}

{
  "event": "run_extended", 
  "user_id": "uuid",
  "run_id": "uuid",
  "before": {"end_date": "2025-06-15", "day_count": 1},
  "after": {"end_date": "2025-06-16", "day_count": 2},
  "triggered_by": "day_mark_created",
  "timestamp": "2025-06-16T09:15:00Z"
}

{
  "event": "run_merged",
  "user_id": "uuid", 
  "merged_run_id": "uuid",
  "source_runs": [{"id": "uuid1", "day_count": 3}, {"id": "uuid2", "day_count": 2}],
  "result_run": {"start_date": "2025-06-10", "end_date": "2025-06-17", "day_count": 8},
  "gap_filled": "2025-06-14",
  "timestamp": "2025-06-14T14:20:00Z"
}
```

### Service Level Indicators (SLIs)
```yaml
# SLI Targets & Alerting
run_calculation_latency_p95: <50ms  # Alert if >100ms
run_calculation_latency_p99: <100ms # Alert if >200ms  
api_response_time_p95: <500ms       # Alert if >750ms
merge_operation_rate: <10/user/day  # Alert if >50 (indicates UX issues)
overlap_violation_count: 0          # Alert on any violations
invariant_check_failures: 0         # Alert on any failures
rebuild_duration_p95: <30s          # Alert if >60s

# Health Check Endpoints
GET /health/runs:
  - Verify no overlapping runs exist
  - Check active run count per user ≤ 1  
  - Validate day_count accuracy for sample of runs
  - Response: 200 OK or 503 Service Unavailable

GET /metrics/runs:
  - run_calculations_total (counter)
  - run_calculation_duration_seconds (histogram)
  - active_runs_gauge (gauge) 
  - invariant_violations_total (counter)
```

### Alerting & Dashboards
```bash
# Alert Conditions
- invariant_violations_total > 0 → P0 (immediate)
- run_calculation_latency_p95 > 100ms → P1 (15min)
- overlap_violation_count > 0 → P1 (immediate)
- rebuild_duration_p95 > 60s → P2 (1hr)

# Dashboard Panels
- Run Calculation Performance (latency percentiles)
- Invariant Health (violation counts, health check status)  
- User Engagement (runs created/extended/merged per day)
- System Load (concurrent calculations, queue depth)
```

---

## Expanded Testing

### Property-Based Testing Framework
```python
# Property-based test generator
@given(
  user_timezone=sampled_from(['UTC', 'America/New_York', 'Asia/Tokyo']),
  checkin_dates=lists(dates(min_value=date(2025,1,1), max_value=date(2025,12,31)), 
                      min_size=1, max_size=365),
  edit_operations=lists(sampled_from(['add', 'remove']), max_size=50),
  backfill_dates=lists(dates(), max_size=20)
)
def test_run_invariants_hold(user_timezone, checkin_dates, edit_operations, backfill_dates):
    # Apply operations in random order
    final_state = apply_operations(checkin_dates, edit_operations, backfill_dates, user_timezone)
    
    # Rebuild from scratch  
    rebuilt_state = rebuild_user_runs(final_state.day_marks, user_timezone)
    
    # Assert invariants
    assert no_overlapping_runs(final_state.runs)
    assert single_active_run(final_state.runs)  
    assert day_count_accuracy(final_state.runs)
    assert deterministic_rebuild(final_state.runs, rebuilt_state.runs)
```

### Fuzz Testing Scenarios
```yaml
# Edge case fuzzing
timezone_transitions:
  - DST spring forward (23-hour days)
  - DST fall back (25-hour days) 
  - User timezone changes mid-run
  - International date line crossings

date_boundaries:  
  - Month boundaries (28/29/30/31 day months)
  - Leap year February 29th transitions
  - Year boundary (Dec 31 → Jan 1)
  - Century boundaries

concurrency_patterns:
  - Simultaneous day markings for same user
  - Rapid extend/merge/split operations
  - Rebuild during active day marking
  - Feature flag toggles during operations

load_scenarios:
  - 1000+ runs per user (heavy user simulation)
  - 10,000 concurrent day markings (traffic spike)
  - 95th percentile: 365 day marking history
  - Pathological: 10,000 single-day runs (worst case merge)
```

---

## API Versioning & Compatibility  

### v2 API Endpoints
```yaml
# Versioned endpoint design
GET /api/v2/runs:
  description: "User run history with pagination"
  params: 
    limit: "default=20, max=100"
    offset: "default=0"
    from_date: "optional date filter"
    to_date: "optional date filter" 
  response: 
    runs: [{"id": "uuid", "start_date": "2025-01-01", "end_date": "2025-01-05", "day_count": 5, "active": false}]
    total_count: 42
    has_more: true

GET /api/v2/runs/active:
  description: "Current active run for user"
  response: 
    200: {"run": {"id": "uuid", "start_date": "2025-06-10", "end_date": "2025-06-15", "day_count": 6, "active": true}}
    200: {"run": null}  # if no active run

GET /api/v2/totals:  
  description: "User statistics summary"
  response:
    total_days: 125
    current_run_days: 6  # 0 if no active run
    longest_run_days: 15
    total_runs: 8
    avg_run_length: 4.2

# Legacy compatibility during transition
GET /api/stats: 
  description: "Legacy endpoint, maps to v2 internally"
  flag_gated: ff.potato.runs_v2
  fallback: "in-memory calculation from day_marks"
```

### Contract Testing
```bash
# Golden user dataset for v1 vs v2 validation
golden_users=(
  "user-simple-case"      # 30 days, 3 runs
  "user-complex-merges"   # 200 days, 15 runs with gaps
  "user-timezone-edge"    # DST transitions, timezone changes
  "user-heavy-history"    # 365 days, maximum complexity
)

# Contract test suite
for user in "${golden_users[@]}"; do
  legacy_response=$(curl /api/stats?user_id=$user)
  v2_response=$(curl /api/v2/totals?user_id=$user)
  
  # Assert equivalent responses (allowing for format differences)
  assert_equivalent "$legacy_response" "$v2_response"
done

# Diff tolerance: 0 for correctness-critical fields
assert_exact_match "total_days"
assert_exact_match "current_run_days"  
assert_exact_match "longest_run_days"
```

---

## Implementation Tweaks

### Idempotent Merge Operations
```sql
-- Explicitly idempotent merge: can be replayed safely
FUNCTION merge_adjacent_runs(user_id, new_date) RETURNS run_id:
  -- Get runs that would be connected by new_date
  adjacent_runs = SELECT id FROM runs 
    WHERE user_id = user_id 
    AND (end_date = new_date - 1 OR start_date = new_date + 1)
    ORDER BY start_date;
  
  IF LENGTH(adjacent_runs) = 0 THEN
    RETURN create_single_day_run(user_id, new_date);
  END IF;
  
  -- Calculate merged boundaries (idempotent - same input produces same output)
  min_start = MIN(start_date) FROM runs WHERE id IN adjacent_runs OR start_date = new_date;
  max_end = MAX(end_date) FROM runs WHERE id IN adjacent_runs OR end_date = new_date;
  total_days = max_end - min_start + 1;
  
  -- Create merged run (or update existing if already merged)
  merged_run = UPSERT runs (user_id, start_date, end_date, day_count, active)
    VALUES (user_id, min_start, max_end, total_days, true)
    ON CONFLICT (user_id, start_date, end_date) DO UPDATE SET updated_at = NOW();
  
  -- Deactivate old runs (idempotent - already inactive runs ignored)  
  UPDATE runs SET active = false WHERE id IN adjacent_runs AND id != merged_run.id;
  
  RETURN merged_run.id;
```

### Short Transaction Boundaries
```sql
-- Narrow transaction scope: only affected runs + new day_mark
BEGIN TRANSACTION;
  -- Lock only user's active/adjacent runs
  SELECT * FROM runs 
  WHERE user_id = ? 
  AND (active = true OR end_date >= ? - 1 AND start_date <= ? + 1)
  FOR UPDATE;
  
  -- Insert day mark
  INSERT INTO day_marks (user_id, local_date, value) VALUES (?, ?, true);
  
  -- Update/merge only adjacent runs (typically 1-3 rows)
  perform_run_calculation(user_id, new_date);
COMMIT TRANSACTION;

-- Total transaction duration target: <10ms
-- Locked rows: <5 per user (active run + max 2 adjacent)
```

### Diagnostic Fields
```sql
-- Optional debugging/metrics fields
ALTER TABLE runs ADD COLUMN last_extended_at TIMESTAMP DEFAULT NOW();
ALTER TABLE runs ADD COLUMN merge_count INTEGER DEFAULT 1; -- How many original runs merged into this one
ALTER TABLE runs ADD COLUMN calculation_duration_ms INTEGER; -- Performance tracking

-- Usage in metrics/debugging
SELECT AVG(calculation_duration_ms) FROM runs WHERE created_at > NOW() - INTERVAL '1 hour';
SELECT user_id, COUNT(*) as runs_with_high_merge_count FROM runs WHERE merge_count > 10 GROUP BY user_id;
```

---

## PHASE 0: Research & Spike (COMPLETE ✅)

**Status:** ✅ COMPLETE  
**Links:** [v2_phase0_research.md](../v2_phase0_research.md), [v2_phase0_completion.md](../v2_phase0_completion.md)

**Error Handling:** This phase follows the Error Handling Clause.

### What We Learned
- **Data Model Decision:** Dedicated runs table chosen over users table columns for scalability and historical data preservation
- **Algorithm Design:** Transaction-based merging with timezone-aware consecutive day detection validated  
- **Performance Benchmarks:** <500ms API response time achievable with <50ms run calculation overhead
- **Edge Cases Identified:** 9 comprehensive test scenarios covering month boundaries, timezone transitions, and error conditions
- **Invariants Established:** Non-overlapping runs, single active run per user, deterministic rebuild properties

### Research Outcomes
- Complete runs table schema with PostgreSQL and SQLite variants
- Production-ready algorithm pseudocode with idempotent operations
- Comprehensive test scenario suite for validation  
- Performance optimization strategy for <500ms API response time
- Edge case handling for timezone boundaries and DST transitions

---

## PHASE 6A-1: Schema Design & Constraints

**Goal:** Draft runs table schema with non-overlap guarantees and active run uniqueness.

### Tasks
- Design runs table schema (columns: id, user_id, span daterange, start_date, end_date, day_count, active)
- Implement PostgreSQL EXCLUDE constraint for non-overlapping spans
- Define uniqueness constraint for single active run per user
- Write invariant validation queries

### Exit Criteria
- Schema draft created and applies cleanly to dev DB
- Non-overlapping constraint verified with INSERT test
- Active run uniqueness enforced

### Evidence to Collect
- Run "\d runs" to confirm schema
- Insert overlapping spans and confirm failure

**Error Handling:** This phase follows the Error Handling Clause.

---

## PHASE 6A-2: Fallback & Cross-DB Strategy

**Goal:** Ensure schema logic works in both Postgres and SQLite.

### Tasks
- Implement trigger-based overlap prevention for SQLite fallback
- Validate equivalent guarantees across Postgres and SQLite
- Add invariant queries for both engines

### Exit Criteria
- SQLite prevents overlapping spans with triggers
- Validation queries confirm no overlaps in either DB

### Evidence to Collect
- Insert overlapping spans in SQLite and confirm failure

**Error Handling:** This phase follows the Error Handling Clause.

---

## PHASE 6A-3: Indexes & Performance

**Goal:** Optimize query performance for run lookups.

### Tasks
- Add indexes for efficient user_id + span lookups
- Benchmark queries for <10ms execution target

### Exit Criteria
- Queries return in <10ms on test dataset
- EXPLAIN ANALYZE shows index usage

### Evidence to Collect
- Run EXPLAIN ANALYZE on run lookups

**Error Handling:** This phase follows the Error Handling Clause.

---

## PHASE 6A-4: Migrations & Rollback

**Goal:** Implement migration scripts and rollback plan.

### Tasks
- Write forward migration for runs table
- Add rollback script to drop runs table + constraints
- Update schema.ts and Drizzle relations
- Test migration/rollback on copy of prod DB

### Exit Criteria
- Migration runs cleanly forward and backward
- No corruption of existing day_marks table
- Rollback plan documented

### Evidence to Collect
- Run "npm run db:push --force" for forward migration
- Run "npm run db:rollback" for rollback

**Error Handling:** This phase follows the Error Handling Clause.

### Proof
- Confirm Phase 6A has been split into 4 sub-phases (6A-1 through 6A-4)
- Each sub-phase includes Goal, Tasks, Exit Criteria, and Evidence to Collect

---

## PHASE 6B-1: Idempotent Core Operations

**Goal:** Implement core idempotent run extend/merge/split operations with robust edge case handling.

### Tasks
- Implement idempotent run extend operation (add consecutive day to existing run)
- Implement idempotent run merge operation (fill gap between two runs)
- Implement idempotent run split operation (remove day from middle of run)
- Design deterministic gap-filling algorithm for consecutive date detection
- Map invariants to specific operation steps and validation checkpoints
- Create operation validation: verify day count accuracy and date continuity

### Exit Criteria
- Run extend operation handles duplicate day marking as no-op
- Run merge operation produces identical results when filling same gap repeatedly
- Run split operation correctly handles removal of start/middle/end dates
- All operations maintain day count accuracy (end_date - start_date + 1)
- Operations preserve data invariants (no overlaps, single active run)

### Evidence to Collect
```sql
-- Test idempotent extend operation
SELECT perform_run_extend('test-user', '2025-06-15');
SELECT perform_run_extend('test-user', '2025-06-15'); -- Should be no-op
SELECT * FROM runs WHERE user_id = 'test-user' ORDER BY start_date;
-- Verify identical results

-- Test gap filling merge
INSERT INTO day_marks VALUES ('test-user', '2025-06-10', true);
INSERT INTO day_marks VALUES ('test-user', '2025-06-12', true);
SELECT perform_run_merge('test-user', '2025-06-11'); -- Fill gap
SELECT * FROM runs WHERE user_id = 'test-user'; -- Should show merged run
```

**Error Handling:** This phase follows the Error Handling Clause.

---

## PHASE 6B-2: Transaction Boundaries

**Goal:** Define precise transaction scoping with narrow locks on affected runs only.

### Tasks
- Design transaction boundary scoping (lock only user's affected runs)
- Map PostgreSQL isolation level requirements (READ COMMITTED validation)
- Define performance targets: <10ms transaction duration, <5 rows locked per operation
- Implement transaction boundary identification for each operation type
- Create transaction scope validation: verify minimal row locking

### Exit Criteria
- Transaction scope limited to user's affected runs (<5 rows typical)
- Transaction duration averages <10ms under normal load
- Row locks acquired only on runs adjacent to new day mark
- Database isolation level prevents phantom reads and race conditions
- Transaction boundaries preserve ACID properties under concurrent access

### Evidence to Collect
```sql
-- Test transaction scope
EXPLAIN (ANALYZE, BUFFERS) SELECT * FROM runs 
WHERE user_id = 'test-user' 
AND (active = true OR end_date >= '2025-06-14' AND start_date <= '2025-06-16')
FOR UPDATE;
-- Should show <5 rows locked

-- Test transaction timing
\timing on
BEGIN;
SELECT perform_run_calculation('test-user', '2025-06-15');
COMMIT;
-- Should complete in <10ms
```

**Error Handling:** This phase follows the Error Handling Clause.

---

## PHASE 6B-3: Concurrency & Locking

**Goal:** Implement per-user serialization strategy with PostgreSQL row-level locking and SQLite application-level mutex for race condition mitigation.

### Tasks
- Design PostgreSQL per-user serialization strategy using row-level locking with `FOR UPDATE`
- Implement application-level mutex for SQLite fallback (SQLite does not support row-level locks)
- Document race condition scenarios and specific mitigation strategies for both database engines
- Implement consistent lock ordering (user_id, then start_date) to prevent deadlocks
- Create concurrent access patterns testing for same-user operations
- Validate row-level locking performance under concurrent load (PostgreSQL)
- Validate application mutex performance under concurrent load (SQLite)

### Exit Criteria
- PostgreSQL row-level locking (FOR UPDATE) performs adequately under concurrent load
- SQLite application-level mutex correctly serializes same-user operations
- Race conditions documented with specific mitigation strategies for both engines
- Consistent lock ordering prevents circular dependencies
- Concurrent same-user operations serialize correctly without conflicts (both PostgreSQL and SQLite)
- Different-user operations run concurrently without blocking

### Evidence to Collect
```sql
-- Test concurrent day marking for same user
-- Terminal 1:
BEGIN; 
SELECT * FROM runs WHERE user_id = 'test-user' FOR UPDATE;
-- Hold transaction open

-- Terminal 2: 
INSERT INTO day_marks (user_id, local_date) VALUES ('test-user', '2025-06-15');
-- Should block until Terminal 1 commits

-- Test different-user concurrency
-- Should NOT block
INSERT INTO day_marks (user_id, local_date) VALUES ('different-user', '2025-06-15');
```

**Error Handling:** This phase follows the Error Handling Clause.

---

## PHASE 6B-4: Deadlock & Stress Testing

**Goal:** Validate deadlock prevention strategy and transaction retry logic under high concurrent load.

### Tasks
- Create transaction retry logic for concurrent day marking attempts
- Design deadlock prevention through consistent lock ordering validation
- Implement stress testing with high concurrent load patterns
- Validate retry logic doesn't create infinite loops under pathological load
- Document deadlock scenarios and prevention strategies

### Exit Criteria
- Deadlock prevention strategy validated through concurrent testing
- Retry logic handles temporary conflicts gracefully
- Stress testing shows no deadlocks under high concurrent load
- Transaction retry logic has bounded retry attempts (max 3 retries)
- Invariant violations impossible under concurrent access patterns

### Evidence to Collect
```bash
# High-concurrency stress testing
wrk -t10 -c100 -d30s -s concurrent_day_marking.lua http://localhost:3000/api/days/2025-06-15/no-drink
# Should complete without deadlocks or invariant violations

# Test retry logic under contention
./stress_test_retries.sh --concurrent-users=50 --operations-per-user=100
# Should show bounded retries, no infinite loops
```

```sql
-- Validate no invariant violations after stress test
SELECT COUNT(*) as overlapping_runs FROM (
  SELECT a.id FROM runs a
  JOIN runs b ON a.user_id = b.user_id AND a.id < b.id
  WHERE a.span && b.span
) q;
-- Should return 0

-- Check for multiple active runs
SELECT user_id, COUNT(*) as active_count FROM runs 
WHERE active = true GROUP BY user_id HAVING COUNT(*) > 1;
-- Should return 0 rows
```

**Rollback Plan:** Disable run calculation in day marking API, preserve day_marks functionality  
**Feature Flag(s):** ff.potato.runs_v2 + default OFF + algorithm implementation gated

**Error Handling:** This phase follows the Error Handling Clause.

---

## PHASE 6C: Backfill/Rebuild

**NOTE:** This phase assumes V2 endpoints and storage methods are prerequisites and already implemented. If not implemented, Phase 6X must be completed first.

**Goal:** Design and implement rebuild_user_runs administrative routine with comprehensive operator usage notes. Support both partial and full rebuilds for data maintenance and recovery scenarios.

### Tasks
- Implement `rebuild_user_runs(user_id, from_date?, to_date?)` function
- Design deterministic algorithm: DELETE old runs → GROUP consecutive dates → INSERT new runs
- Create operator playbook for rebuild scenarios (data corruption, timezone migration, algorithm updates)
- Add partial rebuild capability for performance (rebuild only affected date ranges)
- Implement rebuild validation: compare before/after invariants and user statistics
- Create bulk rebuild functionality for mass data migrations
- Design rebuild monitoring and progress reporting for large datasets
- Add rollback capability: backup runs before rebuild, restore on failure

### Risks/Assumptions  
- Rebuild operation locks user runs for extended period (acceptable for admin operation)
- Deterministic algorithm produces identical results across multiple executions
- Partial rebuilds correctly identify all affected runs without missing dependencies
- Bulk rebuilds complete within maintenance windows (target: <1000 users per hour)

### Exit Criteria (Objective, Testable)
- `rebuild_user_runs()` produces identical results on repeated executions (deterministic)
- Partial rebuilds modify only affected date ranges, preserve unrelated runs
- Bulk rebuild completes for 1000+ users within 1 hour maintenance window
- Rebuild validation detects any data corruption or invariant violations
- Operator playbook covers all common rebuild scenarios with step-by-step procedures
- Rollback mechanism restores previous state within 5 minutes on rebuild failure
- Progress reporting shows ETA and completion percentage for long-running rebuilds

### Evidence to Collect
```sql
-- Test deterministic rebuild
SELECT rebuild_user_runs('test-user-1');
backup_1 = SELECT * FROM runs WHERE user_id = 'test-user-1';

SELECT rebuild_user_runs('test-user-1'); -- Rebuild again  
backup_2 = SELECT * FROM runs WHERE user_id = 'test-user-1';

-- Assert identical results
SELECT * FROM backup_1 EXCEPT SELECT * FROM backup_2; -- Should be empty
SELECT * FROM backup_2 EXCEPT SELECT * FROM backup_1; -- Should be empty

-- Test partial rebuild performance
EXPLAIN ANALYZE SELECT rebuild_user_runs('heavy-user', from_date='2025-06-01', to_date='2025-06-30');
-- Should complete in <1s even for users with 365+ day history
```

```bash
# Bulk rebuild performance test
time rebuild_all_users --batch-size=100 --max-workers=10
# Target: <1 hour for 1000 users

# Operator playbook validation
./rebuild_playbook_test.sh  # Execute all documented procedures
# All steps should complete successfully
```

**Rollback Plan:** Restore from backup runs table, validate data integrity  
**Feature Flag(s):** ff.potato.runs_v2 + default OFF + rebuild operations gated

**Error Handling:** This phase follows the Error Handling Clause.

---

## PHASE 6D: Shadow Read & Diff

**NOTE:** This phase assumes V2 endpoints and storage methods are prerequisites and already implemented. If not implemented, Phase 6X must be completed first.

**Goal:** Implement shadow computation of v2 runs alongside legacy system. Generate comprehensive diff reports and establish alerting thresholds. Create go/no-go checklist for production cutover.

### Tasks
- Implement shadow runs calculation that mirrors v2 algorithm without affecting user experience
- Create diff detection comparing legacy vs v2 calculations for identical user data
- Design diff report format: mismatched day counts, overlapping runs, missing runs, extra runs
- Establish alerting thresholds: 0 diffs required for cutover approval
- Create automated diff validation for golden user dataset (representative user scenarios)
- Implement shadow performance monitoring: track calculation latency and resource usage
- Design go/no-go cutover checklist with objective criteria and stakeholder approval gates
- Create diff resolution playbook for investigating and fixing calculation discrepancies

### Risks/Assumptions
- Shadow calculations don't impact production performance (target: <5% overhead)
- Legacy system calculations remain stable during shadow comparison period
- Diff detection catches all meaningful discrepancies without false positives
- Representative golden user dataset covers all edge cases and usage patterns

### Exit Criteria (Objective, Testable)
- Shadow calculations run continuously for 7 days without impacting user experience
- Diff reports show 0 discrepancies between legacy and v2 calculations for golden user dataset
- Alerting triggers correctly when diff thresholds exceeded (tested with synthetic data)
- Performance overhead of shadow calculations measured at <5% of baseline
- Go/no-go checklist covers all stakeholder concerns with objective pass/fail criteria
- Diff resolution playbook successfully resolves 100% of discovered discrepancies
- Automated validation runs hourly and reports diff status to monitoring dashboard

### Evidence to Collect
```sql
-- Shadow calculation validation with acceptance gates
SELECT user_id, legacy_total_days, v2_total_days, 
       legacy_current_run, v2_current_run,
       legacy_longest_run, v2_longest_run,
       is_active_run
FROM shadow_diff_report 
WHERE (legacy_total_days != v2_total_days 
   OR legacy_current_run != v2_current_run
   OR legacy_longest_run != v2_longest_run);
-- Cutover Gates:
-- Active runs: 0 diffs required
-- Historical runs: <0.1% diffs allowed and must be explained

-- Performance impact measurement  
SELECT AVG(response_time_ms) as baseline FROM api_metrics WHERE date < shadow_start_date;
SELECT AVG(response_time_ms) as with_shadow FROM api_metrics WHERE date >= shadow_start_date;
-- with_shadow should be <1.05 * baseline
```

```bash
# Automated diff validation
./validate_golden_users.sh --shadow-mode=true
# Should report: "PASS: 0 diffs detected across all golden users"

# Go/no-go checklist verification
./cutover_checklist.sh --validate
# Should output: "READY FOR CUTOVER: All criteria met"
```

**Rollback Plan:** Disable shadow calculations, no user impact  
**Feature Flag(s):** ff.potato.runs_v2 + default OFF + shadow computation gated

**Error Handling:** This phase follows the Error Handling Clause.

---

## PHASE 6X: V2 Endpoints & Storage Implementation

**Goal:** Implement and validate V2 API endpoints and storage methods that are prerequisites for shadow computation and cutover phases.

### Tasks
- Implement `/api/v2/runs`, `/api/v2/totals`, `/health/runs` endpoints
- Extend feature flag system with `ff.potato.runs_v2` registration
- Add storage methods: `getRunsForUser`, `getRunsCountForUser`, `getTotalsForUser`, `checkRunOverlaps`, `checkMultipleActiveRuns`, `validateDayCounts`
- Resolve TypeScript compilation errors in storage.ts
- Update IStorage interface with V2 method signatures
- Add proper error handling and authentication middleware to endpoints

### Exit Criteria
- Endpoints respond correctly with feature flag enabled
- curl tests confirm API returns valid data structure
- TypeScript compiles with no blocking errors for storage methods
- Storage methods handle edge cases (empty runs, pagination boundaries)
- All endpoints properly authenticated and feature flag gated
- Authenticated requests to /api/v2/runs and /api/v2/totals return HTTP 200 with correct JSON structures (runs + pagination, totals data)
- Server binds successfully to process.env.PORT (or 3000 fallback) and remains running without premature exit

### Evidence to Collect
- curl responses from all new endpoints showing correct data format
- Successful TypeScript build with 0 diagnostics for storage.ts
- Logs showing `ff.potato.runs_v2` correctly registered at startup
- Test coverage for storage methods with various data scenarios
- Performance benchmarks for endpoint response times

**Rollback Plan:** Remove endpoints and storage methods, disable feature flag registration  
**Feature Flag(s):** ff.potato.runs_v2 + default OFF + endpoint implementation gated

**Error Handling:** This phase follows the Error Handling Clause.

---

## PHASE 6E-Lite: Cutover for Small User Base

**Status:** ✅ COMPLETE (2025-09-07)

**NOTE:** This phase assumes V2 endpoints and storage methods are prerequisites and already implemented via Phase 6X.

**Goal:** Switch production over to v2 runs for a tiny user base (1–a few users) once shadow diff looks healthy.

**Tasks:**
- ✅ Run shadow diff (Phase 6D) until results are stable (no obvious mismatches)
- ✅ Flip the feature flag `ff.potato.runs_v2=true`
- ✅ Manually check `/api/v2/runs`, `/api/v2/totals`, and `/health/runs`
- ✅ Watch logs to confirm no invariant violations appear

**Exit Criteria:**
- ✅ v2 endpoints return correct values for known users
- ✅ `/health/runs` shows no overlaps or multiple active runs
- ✅ Logs show no unexpected errors

**Evidence Collected:**
- ✅ Feature flag ff.potato.runs_v2 permanently enabled in server/feature-flags.ts (default ON)
- ✅ Server startup logs: `[Feature Flag] FF_POTATO_RUNS_V2 = true`
- ✅ Migration validation: runs table (2 records), day_marks table (64 records)
- ✅ Database health checks passed:
  - Overlap violations: 0 (no overlapping runs)
  - Multiple active runs per user: 0 (no users with >1 active run)
- ✅ V2 endpoint validation with authentication:
  - `/api/v2/runs` → HTTP 200: `{"runs":[],"pagination":{"page":1,"limit":20,"totalPages":0,"totalRuns":0,"hasNext":false,"hasPrev":false}}`
  - `/api/v2/totals` → HTTP 200: `{"total_days":0,"longest_run":0,"current_run":0}`
  - `/health/runs` → HTTP 200: `{"healthy":true,"timestamp":"2025-09-07T17:09:37.188Z"}`

**Rollback Plan:**
- If issues occur, toggle `ff.potato.runs_v2=false`
- Rebuild from `day_marks` if needed

**Feature Flag(s):**
- `ff.potato.runs_v2` (default ON, cutover complete)

**Error Handling:** This phase follows the Error Handling Clause.

---

## PHASE 7A-1: Schema & Strategy Lock-in

**Goal:** Implement hybrid totals strategy (real-time current stats + stored monthly aggregates). Create run_totals table and lock in schema.

### Tasks
- Create run_totals table schema with monthly aggregate fields and indexes
- Write forward + rollback migrations
- Validate schema applies cleanly in PostgreSQL and SQLite fallback

### Exit Criteria
- run_totals table exists with correct schema
- Forward/rollback migrations succeed with no corruption

### Evidence to Collect
- `\d run_totals` output confirming schema
- Forward + rollback cycle proof

**Error Handling:** This phase follows the Error Handling Clause.

---

## PHASE 7A-2: Aggregation & Reconciliation

**Goal:** Implement aggregate update procedures and nightly reconciliation job.

### Tasks
- Implement monthly aggregate calculation/update logic
- Add reconciliation job comparing aggregates vs runs
- Create reconciliation_log table and validation queries

### Exit Criteria
- Aggregate recomputation completes within 1h for 1000+ users
- Reconciliation detects inconsistencies and logs them

### Evidence to Collect
- EXPLAIN ANALYZE timing of aggregate queries
- Sample reconciliation_log entries showing detection/correction

**Error Handling:** This phase follows the Error Handling Clause.

---

## PHASE 7A-3: Totals API & Migration (Right-Sized)

**Goal:** Expose totals via API and migrate historical totals data.

### Tasks
- Implement `/api/v2/totals` endpoint (auth + ff.potato.totals_v2 gated)
- Add invalidation strategy when runs change
- Implement migration of totals for existing users
- (Optional, Fast-Follow) Add caching/optimization if response times approach 100ms

### Exit Criteria
- `/api/v2/totals` responds correctly with totals data for authenticated users
- Migration results match real-time totals (validation script PASS)
- (Optional) wrk benchmark <100ms if caching added

### Evidence to Collect
- curl `/api/v2/totals` JSON response with totals data
- Migration validation logs with PASS
- (Optional) wrk benchmark results <100ms if caching added

**Rollback Plan:** Disable ff.potato.totals_v2, fall back to real-time calculation  
**Feature Flag(s):** ff.potato.totals_v2 + default OFF + independent of runs_v2

**Error Handling:** This phase follows the Error Handling Clause.
---

## PHASE 7B-Lite: Minimal Observability (Optional for Small User Base)

**Goal:** Provide lightweight monitoring and logging without full dashboards or scaling infrastructure.

### Tasks
- Ensure `/health/runs` endpoint returns invariant health status within <100ms
- Add structured JSON logging for run operations (create, extend, merge, split)
- Include correlation IDs in logs for debugging and traceability
- Verify logs capture errors with clear messages

### Risks/Assumptions
- Lightweight logging is sufficient for a 1–2 user production system
- Full dashboards/alerts not required until user base scales
- Health check performance is acceptable under current load

### Exit Criteria (Objective, Testable)
- `/health/runs` responds within <100ms and accurately reports invariant status
- At least one run operation log line appears in structured JSON format
- Logs include correlation IDs for request tracing
- Errors are captured in logs with meaningful descriptions

### Evidence to Collect
```bash
# Health check validation
curl -s http://localhost:3000/health/runs | jq
# Should return healthy JSON within 100ms

# Structured log sample
{"timestamp":"2025-09-08T12:00:00Z","event":"run_extended","user_id":"test-user","run_id":"run-uuid","day_count_before":5,"day_count_after":6,"correlation_id":"req-abc123"}
```

**Rollback Plan:** Disable structured logging, preserve core functionality  
**Feature Flag(s):** ff.potato.runs_v2 + lightweight observability features gated

**Error Handling:** This phase follows the Error Handling Clause.

---

## PHASE 7C: Frontend Integration

**Goal:** Implement frontend updates so users can see runs history and totals statistics.

### Tasks
- Build dashboard components for current run, longest run, and total days (MVP scope only)
- Add run history view with pagination consuming /api/v2/runs
- Integrate totals panel consuming /api/v2/totals (defer non-MVP fields: avg_run_length, total_runs)
- Ensure feature flag gating: hide runs/totals UI unless ff.potato.runs_v2 and ff.potato.totals_v2 are enabled
- Implement error handling (loading states, empty states, invariant errors)

### Exit Criteria
- Dashboard shows correct totals for a user with active and historical runs
- Run history page pagination explicitly matches API contract fields (total_count, has_more)
- Feature flags toggle UI visibility correctly
- UI matches API contract fields (id, start_date, end_date, day_count, active, totals)

### Evidence to Collect
- Screenshots of dashboard with totals displayed
- curl/API responses from /api/v2/runs and /api/v2/totals compared with UI render
- Feature flag toggle test: UI hidden when flags OFF, visible when flags ON
- Frontend test IDs confirm totals and run history render correctly
- Frontend automation testing (Cypress or React Testing Library) verifying UI renders correctly

**Rollback Plan:** Hide frontend components behind feature flags, preserve existing calendar functionality  
**Feature Flag(s):** ff.potato.runs_v2 + ff.potato.totals_v2 + UI integration gated

**Error Handling:** This phase follows the Error Handling Clause.

---

## PHASE 8: Property-Based & Fuzz Testing

**Goal:** Implement comprehensive property-based test generators and fuzz testing framework. Create determinism validation and timezone/DST edge case coverage.

### Tasks
- Create property-based test generators for random day marking sequences with gaps and edits
- Implement fuzz testing for timezone edge cases: DST transitions, leap days, month boundaries
- Design determinism tests: verify rebuild_user_runs() produces identical results
- Create load testing scenarios for 95th percentile user history complexity
- Implement concurrency fuzz testing: simultaneous operations, race conditions, deadlock scenarios
- Design pathological case testing: maximum complexity edge cases (1000+ single-day runs)
- Create regression test suite with seed control for reproducible failure investigation
- Implement continuous property testing integration with build pipeline

### Risks/Assumptions
- Property-based test generators cover realistic user behavior patterns
- Fuzz testing reveals edge cases not covered by manual test scenarios
- Test execution time reasonable for continuous integration (<10 minutes full suite)
- Generated test cases provide adequate coverage of complex interaction scenarios

### Exit Criteria (Objective, Testable)
- Property-based tests execute 10,000+ random scenarios without invariant violations
- Fuzz testing covers all timezone edge cases: 12 timezones × 4 DST transitions × 12 months
- Determinism tests verify rebuild produces identical results across 1,000 random user histories
- Load testing validates performance under 95th percentile complexity (365 day markings, 50 runs)
- Concurrency testing handles 100 simultaneous operations per user without deadlocks
- Pathological case testing passes with 1,000 single-day runs (worst-case merge scenario)
- Continuous testing integration catches regressions within 1 hour of code changes
- All discovered edge cases added to regression suite with reproducible test seeds

### Evidence to Collect
```python
# Property-based test validation
@given(user_operations=generate_user_operations(max_days=365, max_operations=1000))
def test_invariants_hold_under_random_operations(user_operations):
    final_state = execute_operations(user_operations)
    assert_no_overlapping_runs(final_state)
    assert_single_active_run(final_state)
    assert_deterministic_rebuild(final_state)
    
# Execute 10,000 iterations - should all pass
```

```bash  
# Fuzz testing execution
./fuzz_timezone_edges.sh --iterations=1000 --timezones=all --dst-transitions=true
# Should complete without failures

# Load testing validation  
./load_test_complex_users.sh --users=100 --days-per-user=365 --concurrent-requests=1000
# Should maintain <500ms p95 response time

# Continuous integration validation
./run_property_tests.sh --timeout=600  # 10 minute limit
# Should complete full test suite within time limit
```

**Rollback Plan:** Disable property testing in CI, maintain manual test coverage  
**Feature Flag(s):** No feature flags required (testing infrastructure only)

**Error Handling:** This phase follows the Error Handling Clause.

---

## Feature Flag Activation Strategy

### Development Testing  
1. Enable `ff.potato.runs_v2=true` in development environment during Phases 6A-6E
2. Enable `ff.potato.totals_v2=true` separately during Phase 7A for independent validation
3. Test all phases incrementally with feature flag controls and rollback procedures
4. Validate runs calculations with comprehensive test data and edge cases

### Production Rollout
1. **Phase 6A-6D:** Deploy with `ff.potato.runs_v2=false` (default OFF), shadow validation only
2. **Phase 6E:** Gradual rollout: internal accounts (1%) → beta users (10%) → general users (100%)  
3. **Phase 7A:** Independent rollout of `ff.potato.totals_v2` after runs_v2 stability established
4. **Monitoring:** Continuous SLI monitoring with automatic rollback triggers
5. **Final Validation:** 7-day stability period before legacy system decommission

### Rollback Procedures
- **Immediate Rollback:** Feature flags disable all v2 functionality instantly (<30 seconds)
- **Data Integrity:** Rebuild capability ensures data recovery from immutable day_marks table
- **Legacy Preservation:** Day marking functionality operates independently of runs features
- **Rollback Testing:** Monthly rollback drills validate procedure effectiveness

---

## Success Metrics

### Functional Requirements
- **Data Accuracy:** 100% invariant compliance across all user scenarios and edge cases
- **Algorithm Correctness:** Deterministic rebuild produces identical results to real-time calculation
- **Feature Completeness:** All planned functionality operates correctly behind feature flags
- **Edge Case Coverage:** Timezone transitions, DST changes, leap years handled correctly

### Performance Requirements  
- **API Response Time:** <500ms p95 for all runs endpoints under production load
- **Run Calculation:** <50ms p95 for extend/merge operations during day marking
- **Database Performance:** <10ms p95 for indexed run queries
- **Rebuild Performance:** <30s p95 for individual user rebuild, <1 hour for bulk operations

### Reliability Requirements
- **Data Consistency:** Zero tolerance for overlapping runs or invariant violations
- **Rollback Capability:** <5 minutes to disable v2 features and restore legacy functionality  
- **Uptime Impact:** <0.1% additional downtime during cutover phases
- **Monitoring Coverage:** 100% of critical operations covered by SLI monitoring and alerting

### User Experience Requirements
- **Transparent Migration:** Users experience no functionality changes during v2 rollout
- **Performance Parity:** v2 system performs identically or better than legacy system
- **Feature Enhancement:** Runs and totals provide meaningful user value without complexity
- **Error Resilience:** Graceful degradation and user-friendly error messages for edge cases